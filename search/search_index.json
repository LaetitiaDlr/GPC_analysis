{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"GPC_analysis","text":"<p>Welcome to the documentation for the <code>GPC_analysis</code> code! Here you will find everything you need to get started with your own Python package.</p> <p>Check out the corresponding \u2b50YouTube tutorial\u2b50 for a video overview!</p>"},{"location":"example_docs/about/changelog.html","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"example_docs/about/changelog.html#001","title":"[0.0.1]","text":""},{"location":"example_docs/about/changelog.html#added","title":"Added","text":"<ul> <li>The initial release!</li> </ul>"},{"location":"example_docs/about/conduct.html","title":"Code of Conduct","text":""},{"location":"example_docs/about/conduct.html#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"example_docs/about/conduct.html#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"example_docs/about/conduct.html#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"example_docs/about/conduct.html#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"example_docs/about/conduct.html#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"example_docs/about/conduct.html#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"example_docs/about/conduct.html#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"example_docs/about/conduct.html#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"example_docs/about/conduct.html#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"example_docs/about/conduct.html#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"example_docs/about/conduct.html#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"example_docs/about/license.html","title":"License","text":"LICENSE.md<pre><code>BSD 3-Clause License\n\nCopyright (c) 2024\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\n   contributors may be used to endorse or promote products derived from\n   this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>"},{"location":"example_docs/code/hints.html","title":"Type Hinting","text":""},{"location":"example_docs/code/hints.html#overview","title":"Overview","text":"<p>In the sample functions provided with the GPC_analysis repository, you will see something like:</p> <pre><code>def make_array(val: float, length: int = 3) -&gt; NDArray:\n</code></pre> <p>If you aren't familiar with type-hinting, that's what the <code>: float</code>, <code>: int</code>, and <code>-&gt; NDArray</code> are indicating. They tell the user what the expected types are for each parameter and return. They are not enforced in any way; they are merely hints (as the name suggests). It is always advisable to use type hints in your code, so get in the habit of doing so!</p> <p>Tip</p> <p>If you have to import a given function solely for type-hinting purposes, you should put it within an <code>if TYPE_CHECKING</code> block (as demonstrated in <code>/src/GPC_analysis/examples/sample.py</code>). It will then only be imported when using a type-checking utility, reducing the overall import time of your module.</p> <p>Note</p> <p>You do not need to touch the <code>py.typed</code> file. It is a marker that Python uses to indicate that type-hinting should be used in any programs that depend on your code.</p>"},{"location":"example_docs/code/hints.html#type-checking","title":"Type Checking","text":"<p>As mentioned, the type hints are just that: hints. If you want to ensure that the types are strictly adhered to across your codebase, you can use mypy to do so. This is a slightly more advanced tool, however, so is not something you need to worry about right now.</p>"},{"location":"example_docs/code/source.html","title":"Source Code","text":""},{"location":"example_docs/code/source.html#adding-your-code","title":"Adding Your Code","text":"<p>All source code (i.e. your various modules, functions, classes, and so on) should be placed in the <code>/src/&lt;MyPackageName&gt;</code> directory. A sample file named <code>examples/sample.py</code> is included here as a representative example, which you should replace.</p> <p>All the code in the <code>src</code> directory can be imported now that you have installed your package.</p> <p>Tip</p> <p>As an example, you can import and use the demonstration GPC_analysis.examples.sample functions as follows:</p> <pre><code>from MyPackageName.examples.sample import add, make_array\n\nprint(add(1, 2))  # 3\nprint(make_array(3, length=4))  # [3, 3, 3, 3]\n</code></pre> <p>Note</p> <p>For any subfolder within <code>src/&lt;MyPackageName&gt;</code> containing Python code, you must have an <code>__init__.py</code> file, which will tell Python that this is a module you can import.</p>"},{"location":"example_docs/code/source.html#docstrings","title":"Docstrings","text":"<p>The code comments beneath each function are called docstrings. They should provide an overview of the purpose of the function, the various parameters, and the return values (if any). Here, we are using the NumPy style docstrings, but you can pick a different style if you like later on.</p>"},{"location":"example_docs/code/source.html#linting-and-formatting","title":"Linting and Formatting","text":"<p>When you installed the <code>[dev]</code> dependencies, you also installed <code>ruff</code>, which is a versatile Python linter to clean up your code. To run <code>ruff</code>, use the following command in the base directory: <code>ruff check --fix</code>. This will also be done automatically on pull requests via the pre-commit CI tool.</p>"},{"location":"example_docs/code/tests.html","title":"Testing","text":""},{"location":"example_docs/code/tests.html#overview","title":"Overview","text":"<p>Writing effective tests for your code is a crucial part of the programming process. It is the best way to ensure that changes you make to your codebase throughout the development process do not break the core functionality of your code. This may be your first time writing tests, but trust me that it is essential.</p>"},{"location":"example_docs/code/tests.html#pytest","title":"Pytest","text":"<p>Put any unit tests in the <code>/tests</code> folder. A sample test (i.e. <code>/tests/sample/examples/test_sample.py</code>) is included as a representative example.</p> <p>Note</p> <p>All your testing scripts should start with <code>test_</code> in the filename.</p> <p>When you installed the package with the <code>[dev]</code> extras, you installed everything you need to run your unit tests. To run the unit tests locally, run <code>pytest .</code> in the base directory. It will let you know if any tests fail and what the reason is for each failure.</p>"},{"location":"example_docs/code/tests.html#code-coverage","title":"Code Coverage","text":"<p>The <code>/.codecov.yml</code> file is a configuration file for Codecov, which will tell you the fraction of lines covered by your test suite if the GitHub integration is enabled.</p> <p>In order for Codecov to work properly, you will need to make a Codecov account and activate it on your newly made repository. You will also need to add the <code>CODECOV_TOKEN</code> secret to your repository.</p>"},{"location":"example_docs/github/commits.html","title":"Saving Your Work","text":"<p>There are still a few more steps left, but at this point you will want to make sure to save your work!</p>"},{"location":"example_docs/github/commits.html#pushing-your-changes","title":"Pushing Your Changes","text":"<p>Commit any changes you've made and push them to your repository</p> <p>If you are using a program like GitKraken, this will involve the following steps:</p> <ol> <li>Save your work.</li> <li>Recommended: make a new branch for your work (e.g. <code>develop</code>)</li> <li>Click \"Stage all changes\".</li> <li>Add a helpful commit message.</li> <li>Commit the changes.</li> <li>Click \"push\".</li> </ol> <p>Tip</p> <p>It is advisable to make changes in a new branch rather than in <code>main</code> so that you can ensure your unit tests pass before the code is merged into the codebase.</p> <p>Then go on GitHub to see your changes. Assuming you pushed your changes to a new branch, you'll likely see a message asking if you want to make a Pull Request to merge in your changes into the <code>main</code> branch.</p>"},{"location":"example_docs/github/workflows.html","title":"GitHub Actions","text":""},{"location":"example_docs/github/workflows.html#workflows","title":"Workflows","text":"<p>The last major piece of the puzzle is GitHub Actions, which is an automated suite of workflows that run every time a commit or pull request is made. The GitHub workflows can be found in the <code>.github/workflows</code> folder.</p>"},{"location":"example_docs/github/workflows.html#tests","title":"Tests","text":"<p>The <code>/.github/workflows/tests.yaml</code> file contains the workflow to have GitHub automatically run the full suite of tests on every commit and pull request. For the most basic case outlined here, you do not need to make any modifications (other than, perhaps, the desired Python versions you wish to test on).</p> <p>By default, the test suite is set up to install the following packages:</p> <pre><code>pip install -r tests/requirements.txt\npip install .[dev]\n</code></pre> <p>As you can see above, it will install specific versions of the dependencies outlined in <code>/tests/requirements.txt</code>. Unlike <code>pyproject.toml</code>, you want to include specific versions here so that your test suite is reproducible.</p> <p>The <code>/.github/dependabot.yml</code> file is set up such that Dependabot will automatically open pull requests to update any versions in your <code>/tests/requirements.txt</code> file as they come out so that your code will always be tested on the newest releases of the various dependencies. This will ensure that your code doesn't break as dependencies update, but if it does, you will know what needs fixing.</p>"},{"location":"example_docs/github/workflows.html#documentation","title":"Documentation","text":"<p>The <code>/.github/workflows/docs.yaml</code> file contains the workflow to have GitHub test the build process for the documentation and deploy it (if enabled).</p> <p>To have your documentation automatically deployed on a GitHub webpage:</p> <ol> <li>Go to the settings page of your repository.</li> <li>Click the \"Pages\" section under \"Code and automation.\"</li> <li>Select \"Deploy from a branch\" under \"Source\"</li> <li>Set the branch to be \"gh-pages\" with \"/ (root)\" as the folder.</li> <li>Wait a minute and refresh the page. You'll see a message that your site is live with a URL to the documentation.</li> </ol> <p></p> <p>Once this process is done, the documentation will be live and will update with each commit.</p>"},{"location":"example_docs/github/workflows.html#release","title":"Release","text":"<p>The <code>/.github/workflows/release.yaml</code> file contains the workflow to have GitHub upload your package to PyPI every time you mint a new release on GitHub. This is a slightly more advanced topic that you can read more about at a later time, but it's there for when you need it.</p>"},{"location":"example_docs/installation/install.html","title":"Pip Installing","text":"<p>Now it's time to install your Python package! You will want to install your Python package in \"editable\" mode, which means you won't have to re-install your code every time you make updates to it. Additionally, you will want to install several optional dependencies (listed under the <code>[project.optional-dependencies]</code> header in <code>pyproject.toml</code>) to ensure that you can test and build the documentation for your code.</p> <p>With all this in mind, you will want to run the following in the command line from the base of the package directory:</p> <pre><code>pip install -e .[dev,docs]\n</code></pre> <p>Here, the <code>-e</code> means editable mode, the <code>.</code> means the current directory, and the <code>[dev,docs]</code> means it will install the \"dev\" and \"docs\" optional dependency set listed in the <code>pyproject.toml</code> file.</p> <p>Tip</p> <p>You should generally start from a clean Python environment, such as a new Conda environment if you are using Anaconda or one of its variants.</p> <p>To make sure you installed your package successfully, open a Python console and run <code>import &lt;MyPackageName&gt;</code>. It should return without any errors. If there are errors, it's likely because you forgot to replace a \"GPC_analysis\" placeholder with the name of your package.</p>"},{"location":"example_docs/installation/pyproject.html","title":"pyproject.toml","text":"<p>The <code>pyproject.toml</code> file contains all of the necessary information on how Python will install your package.</p>"},{"location":"example_docs/installation/pyproject.html#metadata","title":"Metadata","text":"<p>There are several metadata-related fields that you will likely want to update. You should have already updated the <code>name</code> of the package in a prior step when you replaced \"GPC_analysis\" everywhere, but you will also want to change the following:</p> <ul> <li><code>description</code></li> <li><code>license</code> (if you changed the default <code>LICENSE.md</code> file)</li> <li><code>authors</code></li> <li><code>keywords</code></li> </ul> <p>Aside from the <code>name</code>, none of the above are strictly necessary and can be left as-is (or removed) if you are unsure.</p>"},{"location":"example_docs/installation/pyproject.html#dependencies","title":"Dependencies","text":"<p>The most important fields to update are related to the dependencies: the Python packages that your own code relies on. This will ensure that they are automatically installed when installing your Python package.</p> <p>The required dependencies are listed under the <code>[project]</code> header in the <code>dependencies</code> field. By default, the GPC_analysis repository lists <code>[\"numpy\"]</code>. Include any dependencies you want in this list, separated by commas. This should be all the packages you import in your code that are not standard Python libraries.</p> <p>Tip</p> <p>Not sure what dependencies you need just yet? No problem. You can come back to this later.</p> <p>Note</p> <p>If you know a specific minimum version is needed for your code, you should set that here as well (e.g. <code>[\"numpy&gt;=1.23.0\"]</code>). However, only use this when it is necessary so that users aren't restricted to a given version without a valid reason.</p>"},{"location":"example_docs/installation/pyproject.html#python-version","title":"Python Version","text":"<p>If you know your code can only run on certain Python versions, you should specify that in the <code>requires-python</code> field under the <code>[project]</code> header. When in doubt, we recommend setting it to the range of currently supported Python versions (specifically those with security and bugfix statuses).</p> <p>You can also update the listed versions in the <code>classifiers</code> field, although this is only for informational purposes. The list of supported Python classifier fields can be found on the corresponding PyPI page.</p>"},{"location":"example_docs/intro/resources.html","title":"Resources","text":""},{"location":"example_docs/intro/resources.html#software-development","title":"Software Development","text":"<p>Looking for external resources to get started with software development? Here are some useful ones:</p> <ul> <li>Scientific Python Development Guide</li> <li>Turing Way Guide for Reproducible Research</li> <li>Turing Way Guide for Project Design</li> </ul>"},{"location":"example_docs/intro/resources.html#git-and-version-control","title":"Git and Version Control","text":"<p>For git and version control specifically:</p> <ul> <li>Git Guides</li> <li>Software Carpentry</li> </ul> <p>For GitHub:</p> <ul> <li>GitHub Docs</li> <li>GitHub Skills</li> </ul>"},{"location":"example_docs/intro/why.html","title":"Why?","text":""},{"location":"example_docs/intro/why.html#purpose","title":"Purpose","text":"<p>The first question to address is: why? Why use a GPC_analysis repository like this? Why make a Python package at all, as opposed to writing custom scripts or Jupyter Notebooks?</p> <p>The answer, in short, is sustainable and reproducible software development. Here are some of the benefits:</p> <ul> <li>Your package can be easily installed by others using <code>pip</code>.</li> <li>Your package can have automated unit tests that run every time you make a commit, making sure you don't accidentally break your own code.</li> <li>You can easily make and share documentation with no hassle.</li> <li>You will instantly be adopting good programming practices that will help you for life.</li> </ul> <p>Of course, there are many more reasons, but hopefully that's convincing enough!</p>"},{"location":"example_docs/intro/why.html#alternatives","title":"Alternatives","text":"<p>This is by no means the only GPC_analysis of its kind. Some alternatives include:</p> <ul> <li>cookiecutter and cookiecutter-cms</li> <li>pyscaffold</li> <li>python-package-GPC_analysis</li> </ul> <p>... and many more.</p> <p>Feel free to use them if you wish! This GPC_analysis repository exists because we are all opinionated people, and this GPC_analysis focuses on things that I value most. But the point is to just use something that works well for you.</p>"},{"location":"example_docs/mkdocs/build.html","title":"Building the Docs","text":""},{"location":"example_docs/mkdocs/build.html#the-mkdocsyml-file","title":"The <code>mkdocs.yml</code> File","text":"<p>Once you have added your documentation, you will need to update the <code>/mkdocs.yml</code> file with information about how you want to arrange the files. Specifically, you will need to update the <code>nav</code> secction of the <code>mkdocs.yml</code> file to point to all your individual <code>.md</code> files, organizing them by category.</p> <p>Note</p> <p>Keep the <code>- Code Documentation: reference/</code> line in the <code>nav</code> section of <code>mkdocs.yml</code>. It will automatically transform your docstrings into beautiful documentation! The rest of the <code>nav</code> items you can replace.</p>"},{"location":"example_docs/mkdocs/build.html#the-build-process","title":"The Build Process","text":"<p>To see how your documentation will look in advance, you can build it locally by running the following command in the base directory:</p> <pre><code>mkdocs serve\n</code></pre> <p>A URL will be printed out that you can open in your browser.</p>"},{"location":"example_docs/mkdocs/build.html#deploying-the-docs","title":"Deploying the Docs","text":"<p>To allow your documentation to be visible via GitHub Pages, go to \"Settings &gt; Pages\" in your repository's settings and make sure \"Branch\" is set to \"gh-pages\" instead of \"main\".</p>"},{"location":"example_docs/mkdocs/docs.html","title":"Writing the Docs","text":""},{"location":"example_docs/mkdocs/docs.html#mkdocs","title":"Mkdocs","text":"<p>Now it's time to write some documentation! This isn't very difficult, and of course you're reading some documentation right now. The documentation is written using markdown, which is the same way GitHub comments are formatted.</p> <p>Tip</p> <p>Check out the Markdown Guide for an overview of the basic syntax.</p> <p>This GPC_analysis repository uses a documentation format called mkdocs, specifically a useful theme called Material for Mkdocs. This enables many wonderful goodies like the \"tip\" callout you see above and much more.</p>"},{"location":"example_docs/mkdocs/docs.html#adding-markdown-files","title":"Adding Markdown Files","text":"<p>Your documentation will live in the <code>/docs</code> folder. You can think of each markdown (<code>.md</code>) file as being a specific page in the documentation, and each folder as being a related collection of pages. The markdown page you are reading right now is found at <code>/docs/example_docs/mkdocs/docs.md</code>, for instance. Of course, you will want to replce the <code>/docs/example_docs</code> folder with your own documentation.</p> <p>Note</p> <p>You typically do not need to touch the <code>/docs/gen_ref_pages.py</code> script. It is used to automatically build the documentation for your code from its docstrings.</p>"},{"location":"example_docs/setup/basics.html","title":"Initial Changes","text":"<p>At this point, you now have your GPC_analysis repository on GitHub and locally on your machine. Now it's time to start making some modifications.</p>"},{"location":"example_docs/setup/basics.html#readme","title":"README","text":"<p>The first thing to do is update the README (<code>/README.md</code>), which should contain a user-friendly summary of what your package is all about. This can be whatever you want. Feel free to be creative!</p>"},{"location":"example_docs/setup/basics.html#license","title":"License","text":"<p>The GPC_analysis repository comes premade with a sample license (<code>/LICENSE.md</code>), in this case the very popular and permissive BSD 3-Clause license. Feel free to change this for your own project or keep it as-is if you don't quite know yet.</p> <p>Tip</p> <p>There are many licenses that one can consider. A comprehensive list can be found on the Open Source Initiative website, but a less overwhelming route is to use choosealicense.com.</p>"},{"location":"example_docs/setup/basics.html#code-of-conduct","title":"Code of Conduct","text":"<p>The GPC_analysis repository ships with a premade Code of Conduct (<code>/CODE_OF_CONDUCT.md</code>) that is obtained from the Contributor Covenant. Of course, you can feel free to keep or change this as you see fit, but it is often a good idea to have a code of conduct for public repositories.</p>"},{"location":"example_docs/setup/name.html","title":"Updating the Name","text":"<p>Now for your first major task: replace all instances of the word \"GPC_analysis\" with your desired package name.</p> <p>Note</p> <p>Don't forget to update the name of the <code>/src/GPC_analysis</code> folder, e.g. so that it is of the form <code>src/&lt;MyPackageName&gt;</code>.</p> <p>Tip</p> <p>If you're using Visual Studio Code as your editor, you can do <code>ctrl+shift+H</code> to find-and-replace all instances of \"GPC_analysis\" with your own package name.</p> <p></p>"},{"location":"example_docs/setup/prep.html","title":"Preparatory Steps","text":""},{"location":"example_docs/setup/prep.html#naming-your-package","title":"Naming Your Package","text":"<p>So, you have an idea for your own Python package. The first thing you'll need to do is come up with a name!</p> <p>Tip</p> <p>If you plan on making a Python package that is widely distributed, first check to see if the name already exists on PyPI.</p>"},{"location":"example_docs/setup/prep.html#making-a-repository","title":"Making a Repository","text":"<p>With a nice name in mind, create a new repository using this GPC_analysis. Give it a name, a description, and decide if you want it to be public or private.</p> <p></p>"},{"location":"example_docs/setup/prep.html#cloning-your-repository","title":"Cloning Your Repository","text":"<p>You'll now want to clone the repository to your local machine so you can easily make changes.</p>"},{"location":"example_docs/setup/prep.html#via-a-desktop-client","title":"Via a Desktop Client","text":"<p>You can use a desktop client to interface with GitHub. It is worthwhile to learn how to use such a program for your day-to-day work.</p> <p>Tip</p> <p>We strongly suggest using GitKraken to interface with git and GitHub. GitKraken Pro is also free for students.</p> <p></p>"},{"location":"example_docs/setup/prep.html#via-the-command-line","title":"Via the Command Line","text":"<p>If you prefer, you can clone the repository via the following command in the command-line, provided you have git installed.</p> <pre><code>git clone https://github.com/MyAccountName/MyPackageName\n</code></pre> <p>You can get the URL directly from the GitHub page when you click the green \"&lt;&gt; Code\" button.</p>"},{"location":"reference/SUMMARY.html","title":"SUMMARY","text":"<ul> <li>GPC_analysis<ul> <li>class_GPC</li> <li>examples<ul> <li>sample</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/GPC_analysis/class_GPC.html","title":"class_GPC","text":""},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset","title":"GPC_dataset","text":"<pre><code>GPC_dataset(filepath_dict, sample_information, palette=None, report_type='raw', int_x_range=[14, 26], baseline_window=[10, 31], polymer='PP')\n</code></pre> <p>A class to handle GPC dataset analysis and visualization.</p> <p>This class processes GPC (Gel Permeation Chromatography) data from Excel files, performs baseline correction, calculates molecular weight distributions (MMD), and computes Mw, Mn, and PDI values.</p> <p>Parameters:</p> <ul> <li> <code>filepath_dict</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping sample names to their file paths. Example: {'Sample1': 'path/to/file1.xlsx', 'Sample2': 'path/to/file2.xlsx'}</p> </li> <li> <code>sample_information</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping sample names to their metadata. Each sample must have at least an 'Experiment' key. The sample names MUST match the keys in filepath_dict.</p> <p>Required structure: {     'SampleName1': {         'Experiment': str,  # Required - experiment identifier         'Sample': str,      # Optional - sample identifier         # ... other custom fields     },     'SampleName2': {         'Experiment': str,         'Sample': str,     } }</p> <p>Example: {     'P1.022': {         'Experiment': 'DDV418',         'Sample': 'P1.022',         'Milling Time (s)': 3600,         'Beads Type': 'Steel'     } }</p> </li> <li> <code>palette</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary mapping sample names to colors for plotting. If None, a default color palette will be generated. Example: {'Sample1': 'red', 'Sample2': 'blue'}</p> </li> <li> <code>report_type</code>               (<code>str</code>, default:                   <code>'raw'</code> )           \u2013            <p>Type of report to process. Options are: - 'raw' (default): Process raw GPC data with baseline correction - 'excel': Use pre-processed data from Excel report</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If filepath_dict is empty If sample_information is missing required keys If sample names don't match between filepath_dict and sample_information</p> </li> <li> <code>TypeError</code>             \u2013            <p>If inputs are not of the expected types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; filepath_dict = {\n...     'Sample1': 'data/sample1.xlsx',\n...     'Sample2': 'data/sample2.xlsx'\n... }\n&gt;&gt;&gt; sample_info = {\n...     'Sample1': {'Experiment': 'Exp1', 'Sample': 'Sample1'},\n...     'Sample2': {'Experiment': 'Exp1', 'Sample': 'Sample2'}\n... }\n&gt;&gt;&gt; gpc = GPC_dataset(filepath_dict, sample_info, report_type='raw')\n</code></pre> <p>Parameters:</p> <ul> <li> <code>filepath_dict</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping sample names to their file paths. Keys must match those in sample_information.</p> </li> <li> <code>sample_information</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping sample names to metadata dictionaries. Each sample dict must contain at least 'Experiment' key. Keys must match those in filepath_dict.</p> </li> <li> <code>palette</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Custom color palette for plots. If None, auto-generated.</p> </li> <li> <code>report_type</code>               (<code>str</code>, default:                   <code>'raw'</code> )           \u2013            <p>'raw' for raw data processing or 'excel' for pre-processed data. Default is 'raw'.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If inputs fail validation checks.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def __init__(self, filepath_dict, sample_information,\n            palette=None, report_type='raw',\n            int_x_range=[14, 26], baseline_window = [10, 31], \n            polymer = 'PP'):\n    \"\"\"\n    Initialize GPC dataset analysis.\n\n    Parameters\n    ----------\n    filepath_dict : dict\n        Dictionary mapping sample names to their file paths.\n        Keys must match those in sample_information.\n\n    sample_information : dict\n        Dictionary mapping sample names to metadata dictionaries.\n        Each sample dict must contain at least 'Experiment' key.\n        Keys must match those in filepath_dict.\n\n    palette : dict, optional\n        Custom color palette for plots. If None, auto-generated.\n\n    report_type : str, optional\n        'raw' for raw data processing or 'excel' for pre-processed data.\n        Default is 'raw'.\n\n    Raises\n    ------\n    ValueError\n        If inputs fail validation checks.\n    \"\"\"\n    # Validate inputs\n    self._validate_inputs(filepath_dict, sample_information, report_type)\n    # for integration \n    self.int_x_range = int_x_range\n    self.baseline_window = baseline_window\n    self.filepath_dict = filepath_dict\n    self.sample_information = sample_information\n    self.report_type = report_type\n    # Mark-Houwink parameters\n    self.PS_alpha, self.PS_K = 0.722, 0.000102\n    if polymer == 'PP':\n        self.PP_alpha, self.PP_K = 0.725, 0.000190\n        self.density_PP = 910  # g/L\n\n    # Calculate conversion factors\n    self.H_0 = (math.log10(self.PS_K) - math.log10(self.PP_K))/(self.PP_alpha+1)\n    self.H_1 = (self.PS_alpha+1)/(self.PP_alpha+1)\n\n    # Setup color palette\n    self.colors = ['red', 'blue', 'green', 'orange', 'purple', \n                'brown', 'pink', 'gray', 'olive', 'cyan']\n    if palette is None:\n        self.palette = self._create_default_palette()\n    else:\n        self.palette = palette\n\n    # Process data based on report type\n\n\n    if report_type == 'excel':\n        self._process_excel_data()\n    else:  # raw\n        self._process_raw_data()\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.H_0","title":"H_0  <code>instance-attribute</code>","text":"<pre><code>H_0 = (log10(PS_K) - log10(PP_K)) / (PP_alpha + 1)\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.H_1","title":"H_1  <code>instance-attribute</code>","text":"<pre><code>H_1 = (PS_alpha + 1) / (PP_alpha + 1)\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.baseline_window","title":"baseline_window  <code>instance-attribute</code>","text":"<pre><code>baseline_window = baseline_window\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.colors","title":"colors  <code>instance-attribute</code>","text":"<pre><code>colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.density_PP","title":"density_PP  <code>instance-attribute</code>","text":"<pre><code>density_PP = 910\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.filepath_dict","title":"filepath_dict  <code>instance-attribute</code>","text":"<pre><code>filepath_dict = filepath_dict\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.int_x_range","title":"int_x_range  <code>instance-attribute</code>","text":"<pre><code>int_x_range = int_x_range\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.palette","title":"palette  <code>instance-attribute</code>","text":"<pre><code>palette = _create_default_palette()\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.report_type","title":"report_type  <code>instance-attribute</code>","text":"<pre><code>report_type = report_type\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.sample_information","title":"sample_information  <code>instance-attribute</code>","text":"<pre><code>sample_information = sample_information\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.calculate_MMD_from_raw_data","title":"calculate_MMD_from_raw_data","text":"<pre><code>calculate_MMD_from_raw_data(plotting=False)\n</code></pre> <p>Calculate wlogM from raw data. It is calculated as w(logM) = intensity / |d(logM)/d(elution_volume)| and then normalized by the total sum. </p> <p>Parameters:</p> <ul> <li> <code>data_raw</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing raw data (DataFrame) for each sample.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary containing wlogM data (DataFrame) for each sample with logM as index.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def calculate_MMD_from_raw_data(self, plotting = False):\n    \"\"\"\n    Calculate wlogM from raw data. It is calculated as w(logM) = intensity / |d(logM)/d(elution_volume)| and then normalized by the total sum. \n\n    Parameters\n    ----------\n    data_raw : dict\n        Dictionary containing raw data (DataFrame) for each sample.\n\n    Returns\n    -------\n    dict\n        Dictionary containing wlogM data (DataFrame) for each sample with logM as index.\n    \"\"\"\n    data_MMD = {}\n    axintensity = None\n    if plotting:\n        figintensity, axintensity = plt.subplots(figsize=(6, 5))\n    data_corrected = self.raw_data_correction(self.data_converted, columns_to_correct=['Concentration mg/mL'], replace_by=np.nan)\n    for sample_name, df in data_corrected.items():\n        # df_i = df.set_index('LogM').copy()\n        # MMD_i = df_i[['MMD']].copy()\n        # axintensity.plot(MMD_i['MMD'], marker='o', label = sample_name, markersize=1)\n        # data_MMD[sample_name] = MMD_i\n\n        if df.index.name != 'Elution Volume (mL)':\n            df = df.set_index('Elution Volume (mL)')\n        #From intensity now\n        for_MMD = df[['LogM', 'Concentration mg/mL']].copy()\n        #Will take the index as x axis for the gradient (derivative)\n        x = pd.to_numeric(for_MMD.index, errors='coerce')\n        #will now assign a new column _x , drop the NaN and sort the values according to _x\n        for_MMD = for_MMD.assign(_x=x).dropna(subset=['_x']).sort_values('_x')\n\n        dlogMdEV = np.gradient(for_MMD['LogM'].to_numpy(),\n                            for_MMD['_x'].to_numpy())\n        jac = np.abs(dlogMdEV)  # Jacobian d(LogM)/d(elution_volume)\n        jac[jac == 0] = np.finfo(float).eps\n        calculate = for_MMD['Concentration mg/mL'].to_numpy() / jac\n        total = np.nansum(calculate)\n        if total &gt; 0:\n            MMD = calculate / total\n        else:\n            # If total is zero or NaN, return NaNs to keep shape consistent\n            MMD = np.full_like(calculate, np.nan, dtype=float)\n        logM_vals = for_MMD['LogM'].to_numpy()\n        df_MMD = pd.DataFrame({'MMD': MMD}, index=logM_vals)\n        df_MMD.index.name = 'LogM'\n\n        data_MMD[sample_name] = df_MMD\n\n        if plotting and axintensity is not None:\n            axintensity.plot(df_MMD, marker='o', label = sample_name, markersize=1)\n\n    if plotting and axintensity is not None:\n        axintensity.set_xlabel(r'$\\bf{LogM}\\ \\it{(g/mol)}$')\n        axintensity.set_ylabel(r'$\\bf{w(logM)}\\ \\it{(a.u.)}$')\n        axintensity.set_title('from raw data')\n    return data_MMD\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.calculate_Mn_Mw_from_MMD","title":"calculate_Mn_Mw_from_MMD","text":"<pre><code>calculate_Mn_Mw_from_MMD(data_MMD_all)\n</code></pre> <p>Calculate Mw, Mn, PDI, and M_max from MMD data as intensity = MMD.</p> <p>Parameters:</p> <ul> <li> <code>data_MMD_all</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing MMD data (DataFrame) for each sample.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def calculate_Mn_Mw_from_MMD(self, data_MMD_all):\n    \"\"\"Calculate Mw, Mn, PDI, and M_max from MMD data as intensity = MMD.\n    Parameters\n    ----------\n    data_MMD_all : dict\n        Dictionary containing MMD data (DataFrame) for each sample.\"\"\"\n    Mn_Mw_from_MMD = {}        \n    for sample_name, df in data_MMD_all.items():\n        intensity = df['MMD']\n\n        Mi = 10 ** df.index  # Convert logM to M\n        # Mw = sum(Mi**2 *intensity) / sum(Mi*intensity)\n        if sum(intensity) == 0 or sum(intensity/Mi) == 0:\n            print(f\"Warning: Sum of intensity or sum of intensity/Mi is zero for sample {sample_name}. Setting Mw, Mn, PDI, M_max to NaN.\")\n            Mw = np.nan\n            Mn = np.nan\n            PDI = np.nan\n            M_max = np.nan\n        else:\n            Mw = sum(Mi*intensity) / sum(intensity)  # Weight-average molar mass\n            Mn = sum(intensity) / sum(intensity/Mi)  # Number-average molar mass\n            M_max = 10 ** intensity.idxmax() \n            PDI = Mw/Mn\n        Mn_Mw_from_MMD[sample_name] = [Mw, Mn, PDI, M_max]\n    Mn_Mw_from_MMD = pd.DataFrame(Mn_Mw_from_MMD).T\n    Mn_Mw_from_MMD.columns = ['Mw[g/mol]', 'Mn[g/mol]', 'PDI', 'M_max[g/mol]']\n\n    return Mn_Mw_from_MMD\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.calculate_Mn_Mw_raw_data","title":"calculate_Mn_Mw_raw_data","text":"<pre><code>calculate_Mn_Mw_raw_data(xlabel='Experiment', data_raw_corrected=None)\n</code></pre> <p>Calculate Mw, Mn, PDI, and M_max from corrected raw data.</p> <p>Parameters:</p> <ul> <li> <code>data_raw_corrected</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary containing corrected raw data (DataFrame) for each sample. If None, uses self.data_raw_corrected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame containing Mw, Mn, PDI, and M_max for each sample.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def calculate_Mn_Mw_raw_data(self, xlabel='Experiment', data_raw_corrected=None):\n    \"\"\"Calculate Mw, Mn, PDI, and M_max from corrected raw data.\n     Parameters\n     ----------\n     data_raw_corrected : dict, optional\n         Dictionary containing corrected raw data (DataFrame) for each sample. If None, uses self.data_raw_corrected.\n     Returns\n     -------\n     pd.DataFrame\n         DataFrame containing Mw, Mn, PDI, and M_max for each sample.\n     \"\"\"\n    Mn_Mw_from_raw = {}\n    data_raw_corrected = data_raw_corrected.copy() if data_raw_corrected is not None else self.data_raw_corrected.copy()\n    for sample_name, df in data_raw_corrected.items():\n        intensity = df['Concentration mg/mL']\n        logMi = df['LogM']\n        Mi = 10 ** logMi  # Convert logM to M\n        Mw = sum(Mi*intensity) / sum(intensity) if sum(intensity) &gt; 0 else 0  # Weight-average molar mass\n        Mn = sum(intensity) / sum(intensity/Mi) if sum(intensity/Mi) &gt; 0 else 0  # Number-average molar mass\n        M_max = intensity.idxmax()  # M at maximum intensity\n        PDI = Mw/Mn if Mn &gt; 0 else 0\n        info = self.sample_information[sample_name][xlabel]\n        Mn_Mw_from_raw[sample_name] = [Mw, Mn, PDI, M_max]\n    self.Mn_Mw_from_raw = pd.DataFrame(Mn_Mw_from_raw).T\n    self.Mn_Mw_from_raw.columns = ['Mw[g/mol]', 'Mn[g/mol]', 'PDI', 'M_max[g/mol]']\n    return self.Mn_Mw_from_raw\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.convert_ev_to_logM","title":"convert_ev_to_logM","text":"<pre><code>convert_ev_to_logM(data_raw)\n</code></pre> <p>Convert elution volume to logM using calibration data.</p> <p>Parameters:</p> <ul> <li> <code>data_raw</code>               (<code>dict</code>)           \u2013            <p>Dictionary created via extract_raw_data() containing raw data (DataFrame) for each sample.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary containing converted data (DataFrame) for each sample with LogM column added without calibration columns.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def convert_ev_to_logM(self, data_raw):\n    \"\"\"Convert elution volume to logM using calibration data.\n    Parameters\n    ----------\n    data_raw : dict\n        Dictionary created via extract_raw_data() containing raw data (DataFrame) for each sample.\n    Returns\n    -------\n    dict\n        Dictionary containing converted data (DataFrame) for each sample with LogM column added without calibration columns.\n    \"\"\"\n    data_converted = {}\n    for sample_name, df in data_raw.items():\n\n        df = df.copy()\n        df.columns = [c.strip() for c in df.columns]\n\n        # Expected columns\n        req = ['Calib NS Volumes mL', 'Calib LogM Points NS', 'Elution Volume (mL)']\n        missing = [c for c in req if c not in df.columns]\n        if missing:\n            raise KeyError(f\"Missing columns: {missing}. Present columns: {list(df.columns)}\")\n\n        x_calib = df['Calib NS Volumes mL']\n        x_calib = x_calib.dropna()\n        y_calib = df['Calib LogM Points NS']\n        y_calib = y_calib.dropna()\n        coeffs = np.polyfit(x_calib, y_calib, 3)\n\n        ev = df['Elution Volume (mL)']\n        logMPS = coeffs[0]*ev**3 + coeffs[1]*ev**2 + coeffs[2]*ev + coeffs[3]\n        logMPP = self.H_0 + self.H_1 * logMPS\n\n        out = df.drop(columns=['Calib NS Volumes mL', 'Calib LogM Points NS']).copy()\n        out['LogM'] = logMPP\n        data_converted[sample_name] = out\n    return data_converted\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.extract_MMD_from_excel","title":"extract_MMD_from_excel","text":"<pre><code>extract_MMD_from_excel()\n</code></pre> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def extract_MMD_from_excel(self):\n    data_MMD_all = {}\n    for file, path in self.filepath_dict.items():\n        data_MMD_i = pd.read_excel(path, sheet_name='Data MMD', header=0, index_col=0)\n        data_MMD_i = pd.DataFrame(data_MMD_i['MMD'], columns=['MMD'])\n        # Post-processing MMD data: clamp negatives to 0\n        data_MMD_i['MMD'] = data_MMD_i['MMD'].clip(lower=0)\n        data_MMD_all[file] = data_MMD_i\n    return data_MMD_all\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.extract_Mw_Mn_from_excel","title":"extract_Mw_Mn_from_excel","text":"<pre><code>extract_Mw_Mn_from_excel()\n</code></pre> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def extract_Mw_Mn_from_excel(self):\n    data_Mw_Mn_all = {}\n\n    for file, path in self.filepath_dict.items():\n        data_Mw_Mn_i = pd.read_excel(path, sheet_name='Results')\n\n        Mw_i = float(data_Mw_Mn_i.loc[0,\"Unnamed: 6\"])  # type: ignore\n        Mn_i = float(data_Mw_Mn_i.loc[1,\"Unnamed: 6\"])  # type: ignore\n        PDI_i = Mw_i/Mn_i\n        data_Mw_Mn_all[file] = [Mw_i, Mn_i, PDI_i]\n\n    if not data_Mw_Mn_all:\n        print(f\"No files found in filepath_dict\")\n        print(f\"Available files: {self.filepath_dict.keys()}\")\n        return pd.DataFrame()  # Return an empty DataFrame\n    data_Mw_Mn_all = pd.DataFrame(data_Mw_Mn_all).T\n    data_Mw_Mn_all.columns = ['Mw', 'Mn', 'PDI']\n    return data_Mw_Mn_all\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.extract_raw_data","title":"extract_raw_data","text":"<pre><code>extract_raw_data(filepath_dict)\n</code></pre> <p>Extract raw data from Excel files. Extracts concentration, retention volume and calibration data.</p> <p>Parameters:</p> <ul> <li> <code>files</code>               (<code>list</code>)           \u2013            <p>List of file names (reference and samples) to extract data from.</p> </li> <li> <code>filepath</code>               (<code>dict</code>)           \u2013            <p>Dictionary mapping file names to their full paths.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary containing raw data (DataFrame) for each sample.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def extract_raw_data(self, filepath_dict):\n    \"\"\"Extract raw data from Excel files. Extracts concentration, retention volume and calibration data.\n    Parameters\n    ----------\n    files : list\n        List of file names (reference and samples) to extract data from.\n    filepath : dict\n        Dictionary mapping file names to their full paths.\n    Returns\n    -------\n    dict\n        Dictionary containing raw data (DataFrame) for each sample.\n    \"\"\"\n    data_raw_all = {}\n\n    for file, path in filepath_dict.items():\n        # Check if 'Data' sheet exists\n        try:\n            excel_file = pd.ExcelFile(path)\n            available_sheets = excel_file.sheet_names\n\n            if 'Data' not in available_sheets:\n                raise ValueError(\n                    f\"\u274c Sheet 'Data' not found in file: {file}\\n\"\n                    f\"   File path: {path}\\n\"\n                    f\"   Available sheets: {available_sheets}\"\n                )\n\n            data_i = pd.read_excel(path, sheet_name='Data', header=0, index_col=0)\n\n        except FileNotFoundError:\n            raise FileNotFoundError(\n                f\"\u274c File not found: {file}\\n\"\n                f\"   Path: {path}\"\n            )\n        except Exception as e:\n            raise Exception(\n                f\"\u274c Error reading file: {file}\\n\"\n                f\"   Path: {path}\\n\"\n                f\"   Error: {str(e)}\"\n            )\n\n        required_columns = ['Concentration Smoothed ', 'Retention volume processed mL', 'Calib NS Volumes mL', 'Calib LogM Points NS ']\n        missing_columns = [col for col in required_columns if col not in data_i.columns]\n        if missing_columns:\n            raise ValueError(f\"Missing columns in {file}: {missing_columns}\")\n        df_i = data_i[['Concentration Smoothed ', 'Retention volume processed mL', 'Calib NS Volumes mL', 'Calib LogM Points NS ']]\n        df_i.columns = ['Concentration mg/mL', 'Elution Volume (mL)', 'Calib NS Volumes mL', 'Calib LogM Points NS ']\n        df_i = df_i.copy()\n        data_raw_all[file] = df_i\n    return data_raw_all\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.keep_largest_non_nan_block","title":"keep_largest_non_nan_block","text":"<pre><code>keep_largest_non_nan_block(df, col)\n</code></pre> <p>Keep the largest non-NaN block in a DataFrame column. Used after thresholding to retain the main peak.</p> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def keep_largest_non_nan_block(self, df, col):\n    \"\"\"Keep the largest non-NaN block in a DataFrame column. Used after thresholding to retain the main peak.\"\"\"\n    mask = df[col].notna()\n\n    # Find the changes True/False\n    changes = mask.ne(mask.shift())\n    block_ids = changes.cumsum()\n\n    # Keep only the True (non-NaN) blocks\n    valid_blocks = block_ids[mask]\n\n    if len(valid_blocks) == 0:\n        return df.iloc[0:0]  # DataFrame empty\n\n    # Find the largest block\n    largest_block_id = valid_blocks.value_counts().idxmax()\n\n    return df[block_ids == largest_block_id]\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.plotting_MMD_Mw","title":"plotting_MMD_Mw","text":"<pre><code>plotting_MMD_Mw(data_MMD, scale, label1='Experiment', label2=None, label3=None)\n</code></pre> <p>Plotting Molar Mass Distribution (MMD) against Molar Mass (Mw).</p> <p>Parameters:</p> <ul> <li> <code>data_MMD</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing MMD data (DataFrame) for each sample. This is obtained using the function extract_MMD_from_excel.</p> </li> <li> <code>scale</code>               (<code>str</code>)           \u2013            <p>Scale for the x-axis, either 'linear' or 'log'.</p> </li> <li> <code>write_title</code>               (<code>bool</code>)           \u2013            <p>If True, adds a title to the figure, must add a title string as fig_title = --- . Default is False.</p> </li> <li> <code>fig_title</code>               (<code>str</code>)           \u2013            <p>Title of the figure. Default is None.</p> </li> <li> <code>Mn_plotting</code>               (<code>bool</code>)           \u2013            <p>If True, plots vertical lines for Mn values from self.data_Mn_Mw. Default is False.</p> </li> <li> <code>zoom_windows_x</code>           \u2013            <p>Two-element list or tuple containing the x-axis and y-axis limits for zooming. Default is None.</p> </li> <li> <code>filepath_figure_saving</code>               (<code>str</code>)           \u2013            <p>Path where the figure will be saved. Default is None.</p> </li> <li> <code>fig_name_saving</code>               (<code>str</code>)           \u2013            <p>Name of the figure file (without extension). Default is None.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def plotting_MMD_Mw(self, data_MMD, scale, label1 = 'Experiment', label2 = None, label3 = None,):\n\n    \"\"\"\n    Plotting Molar Mass Distribution (MMD) against Molar Mass (Mw).\n\n    Parameters\n    ----------\n    data_MMD : dict\n        Dictionary containing MMD data (DataFrame) for each sample. This is obtained using the function extract_MMD_from_excel.\n    scale : str\n        Scale for the x-axis, either 'linear' or 'log'.\n    write_title : bool, optional\n        If True, adds a title to the figure, must add a title string as fig_title = --- . Default is False.\n    fig_title : str, optional\n        Title of the figure. Default is None.\n    Mn_plotting : bool, optional\n        If True, plots vertical lines for Mn values from self.data_Mn_Mw. Default is False.\n    zoom_windows_x and zoom_windows_y : list or tuple, optional\n        Two-element list or tuple containing the x-axis and y-axis limits for zooming. Default is None.\n    filepath_figure_saving : str, optional\n        Path where the figure will be saved. Default is None.\n    fig_name_saving : str, optional\n        Name of the figure file (without extension). Default is None.\n    \"\"\"\n\n    fig,ax = plt.subplots(figsize=(6, 5))#, dpi=300)\n    #data_MMD_all = {}\n    seen_labels = set()\n\n    for sample_name, df in data_MMD.items():\n        exp_name = self.sample_information[sample_name]['Experiment']\n        df_not_log = 10 ** df.index\n        label_name = str(self.sample_information[sample_name][label1])\n        if label2 is not None:\n            label_name += f' \u2014 {str(self.sample_information[sample_name][label2])}'\n        if label3 is not None:\n            label_name += f' \u2014 {str(self.sample_information[sample_name][label3])}'\n        if label_name not in seen_labels:\n            seen_labels.add(label_name)\n            if type(self.palette) == dict: \n                ax.plot(df_not_log, df['MMD'], label = label_name, color = self.palette[self.sample_information[sample_name]['Experiment']])#, marker = 'o', markersize = 1)\n            elif type(self.palette) == list:\n                ax.plot(df_not_log, df['MMD'], label = label_name, color = self.palette[len(seen_labels)-1])#, marker = 'o', markersize = 1)\n        else:\n            # Cas r\u00e9p\u00e9tition du label: si palette est une liste on reprend l'indice d\u00e9j\u00e0 utilis\u00e9\n            if type(self.palette) == dict:\n                ax.plot(df_not_log, df['MMD'], color = self.palette[self.sample_information[sample_name]['Experiment']])\n            elif type(self.palette) == list:\n                # len(seen_labels)-1 correspond \u00e0 l'indice attribu\u00e9 lors de la premi\u00e8re apparition\n                ax.plot(df_not_log, df['MMD'], color = self.palette[len(seen_labels)-1])\n    ax.set_xscale(scale)\n\n    if scale == 'linear':\n        ax.set_xlabel(r'$\\bf{M}\\ \\it{(g/mol)}$')\n    else:\n        ax.set_xlabel(r'$\\bf{logM}\\ \\it{(g/mol)}$')\n    i=0\n    ax.set_ylabel(r'$\\bf{w(logM)}\\ \\it{(a.u.)}$')\n\n    plt.tight_layout()\n    plt.legend()#bbox_to_anchor=(1.04,0.5), loc='center left')\n    plt.grid(visible=True, which='both', axis='both', linestyle='--', linewidth=0.5)\n\n    return fig, ax\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.plotting_Mw_Mn_boxplot","title":"plotting_Mw_Mn_boxplot","text":"<pre><code>plotting_Mw_Mn_boxplot(data_Mw_Mn_all, unit='g/mol', xlabel='Experiment', label=None, rotation=75)\n</code></pre> <p>Plotting boxplots for Mw and Mn when multiple measurements exist per condition.</p> <p>This function groups data by the specified xlabel field and creates boxplots to show the distribution of molecular weights for each group.</p> <p>Parameters:</p> <ul> <li> <code>data_Mw_Mn_all</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame containing Mw and Mn data for all samples. Should be obtained from calculate_Mn_Mw_raw_data().</p> </li> <li> <code>xlabel</code>               (<code>str</code>, default:                   <code>'Experiment'</code> )           \u2013            <p>Field to group by and use as x-axis labels. Can be 'Milling Time (s)', 'Experiment', 'Mass of PP (g)', etc. Default is 'Experiment'.</p> </li> <li> <code>label</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Field to use for color-coding different groups (e.g., 'Beads Type'). If None, all boxes use default colors. Default is None.</p> </li> <li> <code>rotation</code>               (<code>int</code>, default:                   <code>75</code> )           \u2013            <p>Rotation angle for x-axis labels in degrees. Default is 75.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>fig</code> (              <code>Figure</code> )          \u2013            <p>The figure object</p> </li> <li> <code>ax1</code> (              <code>Axes</code> )          \u2013            <p>Axes for Mw boxplot</p> </li> <li> <code>ax2</code> (              <code>Axes</code> )          \u2013            <p>Axes for Mn boxplot</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Group by milling time, color by bead type\n&gt;&gt;&gt; fig, ax1, ax2 = gpc.plotting_Mw_Mn_boxplot(\n...     gpc.data_Mn_Mw,\n...     xlabel='Milling Time (s)',\n...     label='Beads Type'\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Simple boxplot without color coding\n&gt;&gt;&gt; fig, ax1, ax2 = gpc.plotting_Mw_Mn_boxplot(\n...     gpc.data_Mn_Mw,\n...     xlabel='Experiment',\n...     label=None\n... )\n</code></pre> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def plotting_Mw_Mn_boxplot(self, data_Mw_Mn_all, unit='g/mol', xlabel='Experiment', label=None, rotation=75):\n    \"\"\"\n    Plotting boxplots for Mw and Mn when multiple measurements exist per condition.\n\n    This function groups data by the specified xlabel field and creates boxplots\n    to show the distribution of molecular weights for each group.\n\n    Parameters\n    ----------\n    data_Mw_Mn_all : DataFrame\n        DataFrame containing Mw and Mn data for all samples.\n        Should be obtained from calculate_Mn_Mw_raw_data().\n\n    xlabel : str, optional\n        Field to group by and use as x-axis labels.\n        Can be 'Milling Time (s)', 'Experiment', 'Mass of PP (g)', etc.\n        Default is 'Experiment'.\n\n    label : str, optional\n        Field to use for color-coding different groups (e.g., 'Beads Type').\n        If None, all boxes use default colors.\n        Default is None.\n\n    rotation : int, optional\n        Rotation angle for x-axis labels in degrees. Default is 75.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure object\n    ax1 : matplotlib.axes.Axes\n        Axes for Mw boxplot\n    ax2 : matplotlib.axes.Axes\n        Axes for Mn boxplot\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Group by milling time, color by bead type\n    &gt;&gt;&gt; fig, ax1, ax2 = gpc.plotting_Mw_Mn_boxplot(\n    ...     gpc.data_Mn_Mw,\n    ...     xlabel='Milling Time (s)',\n    ...     label='Beads Type'\n    ... )\n\n    &gt;&gt;&gt; # Simple boxplot without color coding\n    &gt;&gt;&gt; fig, ax1, ax2 = gpc.plotting_Mw_Mn_boxplot(\n    ...     gpc.data_Mn_Mw,\n    ...     xlabel='Experiment',\n    ...     label=None\n    ... )\n    \"\"\"\n    # Calculate data with proper grouping\n    data = self.calculate_Mn_Mw_raw_data(xlabel=xlabel)\n\n    # Group data by xlabel field\n    mw_grouped = data.groupby(data.index.map(lambda x: self.sample_information[x][xlabel]))\n    mn_grouped = data.groupby(data.index.map(lambda x: self.sample_information[x][xlabel]))\n\n    mw_data = mw_grouped['Mw'].apply(list)\n    mn_data = mn_grouped['Mn'].apply(list)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Utiliser la palette pour attribuer une couleur \u00e0 chaque boxplot\n    # On r\u00e9cup\u00e8re les samples correspondant \u00e0 chaque groupe pour trouver la couleur\n    sample_groups = {}  # Pour mapper x_val -&gt; premier sample_name rencontr\u00e9\n    for sample_name in data.index:\n        x_val = self.sample_information[sample_name][xlabel]\n        if x_val not in sample_groups:\n            sample_groups[x_val] = sample_name\n\n    # Plot avec les couleurs de la palette\n    for i, (x_val, mw_vals) in enumerate(mw_data.items()):\n        sample_name = sample_groups[x_val]\n        if unit == 'kg/mol':\n            mw_vals = [val / 1000 for val in mw_vals]\n\n        # D\u00e9terminer la couleur depuis la palette\n        if type(self.palette) == dict:\n            color = self.palette[self.sample_information[sample_name]['Experiment']]\n        elif type(self.palette) == list:\n            color = self.palette[i % len(self.palette)]\n        else:\n            color = \"#461ae4\"  # Couleur par d\u00e9faut\n\n        bp1 = ax1.boxplot([mw_vals], positions=[i], widths=0.4, \n                          patch_artist=True,\n                          boxprops=dict(facecolor=color, color=color, alpha=0.7),\n                          medianprops=dict(color=\"black\", linewidth=2))\n\n    for i, (x_val, mn_vals) in enumerate(mn_data.items()):\n        sample_name = sample_groups[x_val]\n        if unit == 'kg/mol':\n            mn_vals = [val / 1000 for val in mn_vals]\n\n        # D\u00e9terminer la couleur depuis la palette\n        if type(self.palette) == dict:\n            color = self.palette[self.sample_information[sample_name]['Experiment']]\n        elif type(self.palette) == list:\n            color = self.palette[i % len(self.palette)]\n        else:\n            color = \"#e41a1c\"  # Couleur par d\u00e9faut\n\n        bp2 = ax2.boxplot([mn_vals], positions=[i], widths=0.4,\n                          patch_artist=True,\n                          boxprops=dict(facecolor=color, color=color, alpha=0.7),\n                          medianprops=dict(color=\"black\", linewidth=2))\n\n    # Cr\u00e9er une l\u00e9gende si label est sp\u00e9cifi\u00e9\n    if label is not None:\n        from matplotlib.patches import Patch\n        # Collecter les combinaisons uniques de couleur et label\n        unique_combos = {}\n        for x_val in mw_data.index:\n            sample_name = sample_groups[x_val]\n            label_val = str(self.sample_information[sample_name].get(label, 'Unknown'))\n\n            if type(self.palette) == dict:\n                color = self.palette[self.sample_information[sample_name]['Experiment']]\n            elif type(self.palette) == list:\n                idx = list(mw_data.index).index(x_val)\n                color = self.palette[idx % len(self.palette)]\n            else:\n                color = \"#461ae4\"\n\n            if label_val not in unique_combos:\n                unique_combos[label_val] = color\n\n        legend_elements = [Patch(facecolor=color, label=lbl, alpha=0.7) \n                         for lbl, color in unique_combos.items()]\n        ax1.legend(handles=legend_elements, title=label)\n        ax2.legend(handles=legend_elements, title=label)\n\n    # Set labels and ticks\n    ax1.set_ylabel(rf'$\\bf{{Mw}}\\ \\it{{({unit})}}$')\n    ax1.set_xlabel(xlabel)\n    ax1.set_xticks(np.arange(len(mw_data)))\n    ax1.set_xticklabels(mw_data.index, rotation=rotation)\n\n    ax2.set_ylabel(rf'$\\bf{{Mn}}\\ \\it{{({unit})}}$')\n    ax2.set_xlabel(xlabel)\n    ax2.set_xticks(np.arange(len(mn_data)))\n    ax2.set_xticklabels(mn_data.index, rotation=rotation)\n\n    plt.tight_layout()\n    return fig, ax1, ax2\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.plotting_Mw_Mn_scatter","title":"plotting_Mw_Mn_scatter","text":"<pre><code>plotting_Mw_Mn_scatter(data_Mw_Mn_all, xlabel='Experiment', label=None, rotation=75)\n</code></pre> <p>Plotting scatter plots for Mw and Mn.</p> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def plotting_Mw_Mn_scatter(self, data_Mw_Mn_all, xlabel = 'Experiment', label=None, rotation=75):\n    \"\"\"\n    Plotting scatter plots for Mw and Mn.\n    \"\"\"\n    data_Mw_Mn_all = self.calculate_Mn_Mw_raw_data(xlabel=xlabel)\n\n    mw_data = data_Mw_Mn_all['Mw[g/mol]']\n    mn_data = data_Mw_Mn_all['Mn[g/mol]']\n    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(8, 5))\n    x_vals = []\n    i=0\n\n    for exp_name in mw_data.index:\n        # get x-value from sample_information (e.g. Milling Time (s) or Experiment)\n        x_val = self.sample_information[exp_name][xlabel]\n        x_vals.append(x_val)\n\n        # D\u00e9terminer la couleur depuis la palette\n        if type(self.palette) == dict:\n            color = self.palette[self.sample_information[exp_name]['Experiment']]\n        elif type(self.palette) == list:\n            color = self.palette[i % len(self.palette)]\n        else:\n            color = None  # Couleur par d\u00e9faut matplotlib\n\n        # build legend string: \"Experiment \u2014 &lt;label&gt;\"\n        if label != None:\n            exp_str = str(self.sample_information[exp_name].get('Experiment', exp_name))\n            label_val = str(self.sample_information[exp_name].get(label, ''))\n            legend_str = f\"{exp_str} \u2014 {label_val}\"\n\n        # plot points; give each point its legend entry\n            ax1.scatter(x_vals[i], mw_data[exp_name], label=legend_str, color=color, s=50, alpha=0.8)\n            ax2.scatter(x_vals[i], mn_data[exp_name], label=legend_str, color=color, s=50, alpha=0.8)\n        else:\n            ax1.scatter(x_vals[i], mw_data[exp_name], color=color, s=50, alpha=0.8)\n            ax2.scatter(x_vals[i], mn_data[exp_name], color=color, s=50, alpha=0.8)\n        i += 1\n\n    ax1.set_ylabel(r'$\\bf{Mw}\\ \\it{(g/mol)}$')\n    ax2.set_ylabel(r'$\\bf{Mn}\\ \\it{(g/mol)}$')\n    ax1.set_xlabel(f'{xlabel}')\n    ax2.set_xlabel(f'{xlabel}')\n\n    # set xticks using unique ordered values to avoid duplicate-tick issues\n    unique_x = []\n    for xv in x_vals:\n        if xv not in unique_x:\n            unique_x.append(xv)\n    ax1.set_xticks(x_vals)\n    ax1.set_xticklabels(x_vals)\n    ax2.set_xticks(x_vals)\n    ax2.set_xticklabels(x_vals)\n\n    # remap plotted x positions (points were plotted using actual values),\n    # ensure points align with tick indices when ticks are integer positions\n    # if x_vals are already the desired numeric positions, you can skip this remapping.\n    # Here we assume categorical x -&gt; map to integer positions for display consistency\n    # (only affects tick labels; points already at x_vals numeric positions).\n    plt.tight_layout()\n    ax1.tick_params(axis='x', labelrotation=rotation)\n    ax2.tick_params(axis='x', labelrotation=rotation)\n    # show legend only if label parameter is provided\n    if label is not None:\n        ax1.legend()\n        ax2.legend()\n    return fig, ax1, ax2\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.plotting_raw_data","title":"plotting_raw_data","text":"<pre><code>plotting_raw_data(data=None, xlabel='Elution Volume (mL)', ylabel='Concentration mg/mL')\n</code></pre> <p>Plot raw data for each sample.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary containing raw data (DataFrame) for each sample. If None, uses self.data_raw.</p> </li> <li> <code>xlabel</code>               (<code>str</code>, default:                   <code>'Elution Volume (mL)'</code> )           \u2013            <p>Label for the x-axis. Default is 'Elution Volume (mL)'. Can be 'LogM' and will set xscale to log.</p> </li> <li> <code>ylabel</code>               (<code>str</code>, default:                   <code>'Concentration mg/mL'</code> )           \u2013            <p>Label for the y-axis. Default is 'Concentration mg/mL'.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def plotting_raw_data(self, data=None, xlabel='Elution Volume (mL)', ylabel='Concentration mg/mL'):\n    \"\"\"Plot raw data for each sample.\n    Parameters\n    ----------\n    data : dict, optional\n        Dictionary containing raw data (DataFrame) for each sample. If None, uses self.data_raw.\n    xlabel : str, optional\n        Label for the x-axis. Default is 'Elution Volume (mL)'. Can be 'LogM' and will set xscale to log.\n    ylabel : str, optional\n        Label for the y-axis. Default is 'Concentration mg/mL'.\n    \"\"\"\n    if data is None:\n        data = self.data_raw\n\n    fig,ax = plt.subplots(figsize=(6, 5))\n    for sample_name, df in data.items():\n        if xlabel not in df.index.names:\n            df.index = df[xlabel]\n        if xlabel == 'LogM':\n            ax.set_xscale('log')\n        ax.plot(df.index, df[ylabel], label = sample_name)#, color = self.palette[self.label[sample_name]])\n    ax.set_xlabel(f'{xlabel}', fontweight='bold')\n    ax.axvline(x=26, color='black', linestyle='--')\n    ax.axvline(x=31, color='red', linestyle='--')\n    ax.axvline(x=14, color='red', linestyle='--')\n    # ax.set_xlim(1,31)\n    ax.set_ylabel(r'$\\bf{Intensity}\\ \\it{(mg/mL)}$')\n    ax.set_title('Raw data, integration range between red and black lines and baseline calculated between the two red lines')\n    plt.legend()\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.raw_data_correction","title":"raw_data_correction","text":"<pre><code>raw_data_correction(data_extracted, columns_to_correct=['MMD', 'Concentration mg/mL'], average_baseline=True, replace_by=nan, correction_plotting_MMD=False, correction_plotting_intensity=False)\n</code></pre> <p>Correct raw data by baseline subtraction and thresholding. The thresholding is done based on the noise level in the baseline region, keeping only the largest non-NaN block after thresholding.</p> <p>Parameters:</p> <ul> <li> <code>data_extracted</code>               (<code>dict</code>)           \u2013            <p>Dictionary containing extracted data (DataFrame) for each sample.</p> </li> <li> <code>columns_to_correct</code>               (<code>list</code>, default:                   <code>['MMD', 'Concentration mg/mL']</code> )           \u2013            <p>List of column names to apply the correction on.</p> </li> <li> <code>average_baseline</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, average the y-values around the baseline points within a small window (0.5).</p> </li> <li> <code>x_range</code>               (<code>list or tuple</code>)           \u2013            <p>Two-element list or tuple containing the x-axis limits for the main data range.</p> </li> <li> <code>baseline_window</code>               (<code>list or tuple</code>)           \u2013            <p>Two-element list or tuple containing the x-axis limits for the baseline points.</p> </li> <li> <code>replace_by</code>               (<code>float or nan</code>, default:                   <code>nan</code> )           \u2013            <p>Value to replace data points below the threshold. Default is np.nan.</p> </li> <li> <code>correction_plotting_MMD</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, plot the MMD correction, with the raw, corrected curves and the baseline.</p> </li> <li> <code>correction_plotting_intensity</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, plot the intensity correction process, with the raw, corrected curves and the baseline.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary containing corrected data (DataFrame) for each sample.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def raw_data_correction(self, data_extracted, columns_to_correct = ['MMD', 'Concentration mg/mL'], average_baseline = True, \n                        # threshold_at = 0.003, \n                        replace_by = np.nan, \n                        correction_plotting_MMD = False, correction_plotting_intensity = False):\n    \"\"\"Correct raw data by baseline subtraction and thresholding. The thresholding is done based on the noise level in the baseline region, keeping only the largest non-NaN block after thresholding.\n    Parameters\n    ----------\n    data_extracted : dict\n        Dictionary containing extracted data (DataFrame) for each sample.\n    columns_to_correct : list\n        List of column names to apply the correction on.\n    average_baseline : bool, optional\n        If True, average the y-values around the baseline points within a small window (0.5).\n    x_range : list or tuple, optional\n        Two-element list or tuple containing the x-axis limits for the main data range.\n    baseline_window : list or tuple, optional\n        Two-element list or tuple containing the x-axis limits for the baseline points.\n    replace_by : float or np.nan, optional\n        Value to replace data points below the threshold. Default is np.nan.\n    correction_plotting_MMD : bool, optional\n        If True, plot the MMD correction, with the raw, corrected curves and the baseline.\n    correction_plotting_intensity : bool, optional\n        If True, plot the intensity correction process, with the raw, corrected curves and the baseline.\n    Returns\n    -------\n    dict\n        Dictionary containing corrected data (DataFrame) for each sample.\"\"\"\n    data_corrected = {}\n    axbaseline_MMD = None\n    axbaseline_intensity = None\n    x_range = self.int_x_range\n    baseline_window = self.baseline_window\n\n    if correction_plotting_MMD:\n        figbaseline_MMD, axbaseline_MMD = plt.subplots(figsize=(6, 5))\n    if correction_plotting_intensity:\n        figbaseline_intensity, axbaseline_intensity = plt.subplots(figsize=(6, 5))\n\n    for sample_name, df in data_extracted.items():\n        df_i = df.copy()\n        df_i = df_i.set_index('Elution Volume (mL)')\n\n        # df_i = df_i[(df_i.index &gt;= min(x_range)) &amp; (df_i.index &lt;= max(x_range))]\n        baseline = None  # Initialize baseline outside the loop\n        threshold = None  # Initialize threshold outside the loop\n\n        for col in columns_to_correct:\n        # making the baseline correction\n            baseline = self.straight_line_2points(baseline_window, df_i[col], average=average_baseline)\n            # baseline = baseline[(baseline.index &gt;= min(x_range)) &amp; (baseline.index &lt;= max(x_range))]\n\n            df_i[col] = df_i[col] - baseline['y']   # Correcting the data by subtracting the baseline\n            df_i.loc[df_i[col] &lt; 0, col] = 0 # set negative values to 0\n            df_noise = df_i[(df_i.index &gt;= min(baseline_window)) &amp; (df_i.index &lt;= min(x_range))] #looking for the noise in the first minute of elution\n\n            df_i = df_i[(df_i.index &gt;= min(x_range)) &amp; (df_i.index &lt;= max(x_range))]\n            if correction_plotting_intensity and axbaseline_intensity is not None:\n                axbaseline_intensity.plot(df_i.index, df_i[col], linestyle='-')\n            max_noise = abs(df_noise[col].max())# * 1.5\n            threshold = max_noise \n\n            df_i.loc[df_i[col] &lt; threshold, col] = replace_by\n            df_i = self.keep_largest_non_nan_block(df_i, col)\n\n        if correction_plotting_MMD and baseline is not None and threshold is not None and axbaseline_MMD is not None:\n            # axbaseline_MMD.plot(df_i.index, df_i['MMD'], label = sample_name)\n            baseline_filtered = baseline[(baseline.index &gt;= min(x_range)) &amp; (baseline.index &lt;= max(x_range))]\n            axbaseline_MMD.plot(baseline_filtered.index, baseline_filtered['y'], linestyle='--', alpha=0.5)\n            axbaseline_MMD.set_xlabel(r'$\\bf{Elution\\ time}\\ \\it{(min)}$')\n            axbaseline_MMD.set_ylabel(r'$\\bf{MMD}\\ \\it{(a.u.)}$')\n            axbaseline_MMD.set_title(f'MMD Raw data corrected with threshold at {threshold} of max')\n\n        if correction_plotting_intensity and baseline is not None and threshold is not None and axbaseline_intensity is not None:\n            baseline_filtered = baseline[(baseline.index &gt;= min(x_range)) &amp; (baseline.index &lt;= max(x_range))]\n            axbaseline_intensity.plot(df_i.index, df_i['Concentration mg/mL'], linestyle='--')\n            axbaseline_intensity.plot(baseline_filtered.index, baseline_filtered['y'], linestyle='--', alpha=0.5)\n            axbaseline_intensity.set_xlabel(r'$\\bf{Elution\\ time}\\ \\it{(min)}$')\n            axbaseline_intensity.set_ylabel(r'$\\bf{Intensity}\\ \\it{(mg/mL)}$')\n            axbaseline_intensity.set_title(f'Raw data corrected with threshold at {threshold} of max')\n\n            axbaseline_intensity.axhline(y = threshold, color='red', linestyle='--', label = 'threshold')\n        data_corrected[sample_name] = df_i\n    return data_corrected\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.save_results_to_csv","title":"save_results_to_csv","text":"<pre><code>save_results_to_csv(filepath_saving)\n</code></pre> <p>Save results to separate CSV files: - One CSV per sample for MMD data - One CSV for all Mw/Mn results - One CSV per sample for raw corrected data</p> <p>Parameters:</p> <ul> <li> <code>filepath_saving</code>               (<code>str</code>)           \u2013            <p>Directory path where to save the CSV files</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def save_results_to_csv(self, filepath_saving):\n    \"\"\"\n    Save results to separate CSV files:\n    - One CSV per sample for MMD data\n    - One CSV for all Mw/Mn results\n    - One CSV per sample for raw corrected data\n\n    Parameters\n    ----------\n    filepath_saving : str\n        Directory path where to save the CSV files\n    \"\"\"\n    # Cr\u00e9er le dossier s'il n'existe pas\n    if not os.path.exists(filepath_saving):\n        os.makedirs(filepath_saving)\n\n    # Sauvegarder les MMD (un fichier par \u00e9chantillon)\n    mmd_folder = os.path.join(filepath_saving, 'MMD')\n    if not os.path.exists(mmd_folder):\n        os.makedirs(mmd_folder)\n\n    for sample_name, df in self.data_MMD_all.items():\n        safe_name = \"\".join(x for x in sample_name if x.isalnum() or x in \"._- \")\n        file_path = os.path.join(mmd_folder, f\"MMD_{safe_name}.csv\")\n        df.to_csv(file_path)\n\n    # Sauvegarder les Mw, Mn, PDI (un seul fichier)\n    if hasattr(self, 'data_Mn_Mw'):\n        mw_mn_path = os.path.join(filepath_saving, 'Mw_Mn_Results.csv')\n\n        # Si le fichier existe d\u00e9j\u00e0, charger les donn\u00e9es existantes\n        if os.path.exists(mw_mn_path):\n            existing_data = pd.read_csv(mw_mn_path, index_col=0)\n            # Combiner avec les nouvelles donn\u00e9es (les nouvelles \u00e9crasent les anciennes pour les m\u00eames index)\n            combined_data = pd.concat([existing_data, self.data_Mn_Mw])\n            # Supprimer les duplicats en gardant la derni\u00e8re occurrence (les nouvelles donn\u00e9es)\n            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]\n            combined_data.to_csv(mw_mn_path)\n        else:\n            # Si le fichier n'existe pas, cr\u00e9er un nouveau fichier\n            self.data_Mn_Mw.to_csv(mw_mn_path)\n\n    # Sauvegarder les donn\u00e9es brutes corrig\u00e9es (un fichier par \u00e9chantillon)\n    if hasattr(self, 'data_raw_corrected'):\n        raw_folder = os.path.join(filepath_saving, 'Raw_Corrected')\n        if not os.path.exists(raw_folder):\n            os.makedirs(raw_folder)\n\n        for sample_name, df in self.data_raw_corrected.items():\n            safe_name = \"\".join(x for x in sample_name if x.isalnum() or x in \"._- \")\n            file_path = os.path.join(raw_folder, f\"Raw_{safe_name}.csv\")\n            df.to_csv(file_path)\n\n    print(f\"Results saved in {filepath_saving}\")\n</code></pre>"},{"location":"reference/GPC_analysis/class_GPC.html#GPC_analysis.class_GPC.GPC_dataset.straight_line_2points","title":"straight_line_2points","text":"<pre><code>straight_line_2points(x_range, df, average=True)\n</code></pre> <p>Generate a straight line between two points defined by x_range in the DataFrame df for baseline correction.</p> <p>Parameters:</p> <ul> <li> <code>x_range</code>               (<code>list or tuple</code>)           \u2013            <p>A list or tuple containing two x-values [x1, x2] to define the points for the straight line.</p> </li> <li> <code>df</code>               (<code>DataFrame or Series</code>)           \u2013            <p>DataFrame or Series containing the data with x-values as index.</p> </li> <li> <code>average</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, average the y-values around x1 and x2 within a small window (0.5).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>A DataFrame containing the y-values of the straight line at each x-value in df.</p> </li> </ul> Source code in <code>GPC_analysis/class_GPC.py</code> <pre><code>def straight_line_2points(self, x_range, df, average = True):\n\n    \"\"\"Generate a straight line between two points defined by x_range in the DataFrame df for baseline correction.\n    Parameters\n    ----------\n    x_range : list or tuple\n        A list or tuple containing two x-values [x1, x2] to define the points for the straight line.\n    df : pd.DataFrame or pd.Series\n        DataFrame or Series containing the data with x-values as index.\n    average : bool, optional\n        If True, average the y-values around x1 and x2 within a small window (0.5).\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the y-values of the straight line at each x-value in df.\"\"\"\n\n    if len(x_range) == 0 :#or df.isna().all():\n        raise ValueError(\"x_range or df values is empty or is all NaN.\")\n    x1, x2 = min(x_range), max(x_range)\n    if average:\n        range_x1 = df.index[(df.index &gt;= x1 - 0.5) &amp; (df.index &lt;= x1 + 0.5)]\n        range_x2 = df.index[(df.index &gt;= x2 - 0.4) &amp; (df.index &lt;= x2 + 0.4)]\n        average_x1 = np.mean(range_x1)\n        average_x2 = np.mean(range_x2)\n\n        idx1 = (pd.Series(df.index) - average_x1).abs().idxmin() # find index of the closest value to average_x1\n        idx2 = (pd.Series(df.index) - average_x2).abs().idxmin() # find index of the closest value to average_x2\n    else:\n        idx1 = (pd.Series(df.index) - x1).abs().idxmin() # find index of the closest value to x1\n        idx2 = (pd.Series(df.index) - x2).abs().idxmin() # find index of the closest value to x2\n\n    # Get the actual x and y values matching these indices\n    x1_real = df.index[idx1]\n    y1 = df.iloc[idx1]#, 0]\n    x2_real = df.index[idx2]\n    y2 = df.iloc[idx2]#, 0]\n\n    # Line equation calculation : y = ax + b\n    a = (y2 - y1) / (x2_real - x1_real)\n    b = y1 - a * x1_real\n\n    # Generate line points\n    x_vals = df.index.values\n    y_vals = a * x_vals + b\n\n    # Return the line as a DataFrame or Series\n    straight_line = pd.DataFrame({'y': y_vals}, index=df.index)\n    return straight_line\n</code></pre>"},{"location":"reference/GPC_analysis/examples/sample.html","title":"sample","text":""},{"location":"reference/GPC_analysis/examples/sample.html#GPC_analysis.examples.sample.add","title":"add","text":"<pre><code>add(a: float, b: float) -&gt; float\n</code></pre> <p>A function that adds two numbers.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>First number to add.</p> </li> <li> <code>b</code>               (<code>float</code>)           \u2013            <p>Second number to add.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The sum of a and b.</p> </li> </ul> Source code in <code>GPC_analysis/examples/sample.py</code> <pre><code>def add(a: float, b: float) -&gt; float:\n    \"\"\"\n    A function that adds two numbers.\n\n    Parameters\n    ----------\n    a\n        First number to add.\n    b\n        Second number to add.\n\n    Returns\n    -------\n    float\n        The sum of a and b.\n    \"\"\"\n    return a + b\n</code></pre>"},{"location":"reference/GPC_analysis/examples/sample.html#GPC_analysis.examples.sample.divide","title":"divide","text":"<pre><code>divide(a: float, b: float) -&gt; float\n</code></pre> <p>A function that divides two numbers, i.e. a/b.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>float</code>)           \u2013            <p>The numerator</p> </li> <li> <code>b</code>               (<code>float</code>)           \u2013            <p>The denominator</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The value for a/b</p> </li> </ul> Source code in <code>GPC_analysis/examples/sample.py</code> <pre><code>def divide(a: float, b: float) -&gt; float:\n    \"\"\"\n    A function that divides two numbers, i.e. a/b.\n\n    Parameters\n    ----------\n    a\n        The numerator\n    b\n        The denominator\n\n    Returns\n    -------\n    float\n        The value for a/b\n    \"\"\"\n    if b == 0:\n        raise ValueError(\"Uh oh! The value for b should not be 0.\")\n\n    return a / b\n</code></pre>"},{"location":"reference/GPC_analysis/examples/sample.html#GPC_analysis.examples.sample.make_array","title":"make_array","text":"<pre><code>make_array(val: float, length: int = 3) -&gt; NDArray\n</code></pre> <p>A function to transform a number into a numpy array.</p> <p>Parameters:</p> <ul> <li> <code>val</code>               (<code>float</code>)           \u2013            <p>Number to turn into an array.</p> </li> <li> <code>length</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The length of the array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray</code>           \u2013            <p>An array composed of <code>val</code>.</p> </li> </ul> Source code in <code>GPC_analysis/examples/sample.py</code> <pre><code>def make_array(val: float, length: int = 3) -&gt; NDArray:\n    \"\"\"\n    A function to transform a number into a numpy array.\n\n    Parameters\n    ----------\n    val\n        Number to turn into an array.\n    length\n        The length of the array.\n\n    Returns\n    -------\n    NDArray\n        An array composed of `val`.\n    \"\"\"\n    return np.array([val] * length)\n</code></pre>"}]}